{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cabc73e",
   "metadata": {},
   "source": [
    "# Chezz Fine‑Tuning Notebook\n",
    "\n",
    "This notebook demonstrates how to prepare data and fine‑tune **TinyLlama** on chess move prediction using LoRA adapters.  \n",
    "It is reorganized and fully annotated so you can reproduce the results end‑to‑end or adapt it to your own chess dataset.\n",
    "\n",
    "Run the cells sequentially **top‑to‑bottom** on Google Colab (GPU recommended).  \n",
    "If you have questions, feel free to comment on the GitHub repo!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5f539",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Runtime and libraries (6 min)](#runtime-and-libraries-6-min)\n",
    "2. [Mount Drive for automatic resumability (1 min)](#mount-drive-for-automatic-resumability-1-min)\n",
    "3. [⌗ 3  Upload your 500 k file](#3-upload-your-500-k-file)\n",
    "4. [Patch the tokenizer  ► Code](#patch-the-tokenizer-code)\n",
    "5. [Load rows → prompt / completion pairs  ► Code](#load-rows-prompt-completion-pairs-code)\n",
    "6. [Tokenise with masking  ► Code](#tokenise-with-masking-code)\n",
    "7. [Build TinyLlama + LoRA adapters  ► Code](#build-tinyllama-lora-adapters-code)\n",
    "8. [Training arguments & Trainer  ► Code](#training-arguments-trainer-code)\n",
    "9. [(Opt.) peek at generation every 1000 steps  ► Code](#opt-peek-at-generation-every-1000-steps-code)\n",
    "10. [Train (or resume)  ► Code](#train-or-resume-code)\n",
    "11. [Save the first stage adapter](#save-the-first-stage-adapter)\n",
    "12. [Import libraries](#import-libraries)\n",
    "13. [Paths & knobs](#paths-knobs)\n",
    "14. [Tokenizer & special tokens](#tokenizer-special-tokens)\n",
    "15. [Prompt header](#prompt-header)\n",
    "16. [Helpers to build prompt/completion](#helpers-to-build-prompt-completion)\n",
    "17. [Load & oversample datasets](#load-oversample-datasets)\n",
    "18. [Token-level packing](#token-level-packing)\n",
    "19. [Load previous LoRA checkpoint](#load-previous-lora-checkpoint)\n",
    "20. [TrainingArguments](#trainingarguments)\n",
    "21. [peek callback](#peek-callback)\n",
    "22. [Trainer & launch](#trainer-launch)\n",
    "23. [Save to adapter stage 2](#save-to-adapter-stage-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cbd70",
   "metadata": {},
   "source": [
    "## Runtime and libraries (6 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62416b6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71198,
     "status": "ok",
     "timestamp": 1745720293313,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "96zaYHC1nuyu",
    "outputId": "b05586f7-a0fc-45cd-9379-3c477bd010aa"
   },
   "outputs": [],
   "source": [
    "# Shell commands / package installation\n",
    "!pip install \\\n",
    "  transformers \\\n",
    "  peft \\\n",
    "  accelerate \\\n",
    "  bitsandbytes \\\n",
    "  triton \\\n",
    "  datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad6178a",
   "metadata": {},
   "source": [
    "## Mount Drive for automatic resumability (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4de92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 20364,
     "status": "ok",
     "timestamp": 1745720338204,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "oZn2FnMdrxWI",
    "outputId": "2264ea0d-c236-4247-fd29-770473581198"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "PROJECT_DIR = \"/content/drive/MyDrive/chezz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1aeeb9",
   "metadata": {},
   "source": [
    "## ⌗ 3  Upload your 500 k file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c4656",
   "metadata": {
    "id": "_DCe6_MTlkey"
   },
   "outputs": [],
   "source": [
    "# if the file is already on Drive:\n",
    "!cp \"/content/drive/MyDrive/chezz/data/train_500k.jsonl.gz\" /content/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898608c",
   "metadata": {},
   "source": [
    "## Patch the tokenizer  ► Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde333b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "d2202f8a955f45a0aa14ddf2daea7993",
      "8762ed6c236d45d3aa30798a762b142e",
      "ff41b1b0e54e40878224d2875536bda9",
      "19162adb44d24c87b314d721a77adcfa",
      "d48bd27e49074f468efa4b9a5adc2e23",
      "a7f080514dc84992864474ca20d543bf",
      "336391ac7bb941be99bb82ff2a9cd70f",
      "365950683051438389b103d9fb509171",
      "9e710e41720f416e9d8e4db755a5796f",
      "36e0f33dd1bb4cc99df53c786723c09f",
      "b41403dddb4447c1b3d41b4e0ee07346",
      "d20dc6fa02d34ae898e1bab264b4871e",
      "06481d10ead74024a28a374481886842",
      "f6d151af5ed042eba0f469f2dee157f9",
      "32410eb8c3634cb5a34c7699cd6ec59d",
      "f312e570575c4ee8824a52ea824474c8",
      "ea2c6b111e2741ebbd35a751312dd59d",
      "2dfaa87504dc4497bc3d4e35a4b99854",
      "6805d0a70db44b4a908727b9513cea48",
      "242c160e27ea42b8bf63bd231a81372d",
      "1c32ba5a26514cb28c144997988cd45d",
      "72516dc5cc19402282e60dd6f1bc5f19",
      "9b401204d2564fc8b85e44af47907fbc",
      "511dc97bc89e446c9feb255f6b4dc435",
      "867ade58989b46248f7951ff31df9ac6",
      "ef8297ea8f1c493e85bccae62a0c94af",
      "38b6205095614168bf24ba08ba377702",
      "6dc5a3422a8f4e4c9be39dde8d714d1c",
      "8d4c585191f24c35a6f2248c0ba3006c",
      "2b30a4d877454a548b689eab6d0c3b90",
      "2d0d39d2d60441de916d1e638a71c3cd",
      "667a5debe5a847ccbfd5d1eb44f793ec",
      "4fb5202a7f7b49c6b3cdce746d4b55f4",
      "436a62b9dbcb49a7b195722c6ff3029b",
      "c074c741d31140129ad4d3a705996d78",
      "0429793ec5c64b7d8494abcfcaa532a9",
      "161c14b2c6f146ef9a7b2b27a916250b",
      "1c85c90da58e44c388f31ca0d24b1f0e",
      "8af71d48bb4f432db135195820baa9ea",
      "279de376c4cf4083bb556bb361ee51ca",
      "326ec43851fd4962af671cab42145fbf",
      "1a88a25461fd40dea5a633237c4a69e0",
      "e72e93224511464d93027b91a3175847",
      "b8d3f9ae767e4d2e84d2062e22e3824e"
     ]
    },
    "executionInfo": {
     "elapsed": 10083,
     "status": "ok",
     "timestamp": 1745447760957,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "GNZns34-oRbo",
    "outputId": "986beca4-d141-40cd-d927-a26d286ab99f"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tok.pad_token    = tok.eos_token        # TinyLlama has no PAD by default\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "SPECIALS = {\"additional_special_tokens\": [\"<|json|>\", \"</|json|>\"]}\n",
    "tok.add_special_tokens(SPECIALS)\n",
    "end_json_id = tok.convert_tokens_to_ids(\"</|json|>\")   # keep for generate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6226c59b",
   "metadata": {},
   "source": [
    "## Load rows → prompt / completion pairs  ► Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d03841",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "a35676a1bd9f44b69ef1ec0f6323411f",
      "bf493c5f72174d9f8ccc1e3cb6992733",
      "a5b3feb7454d42f5b6e6a571523b0ac3",
      "c319f4985604481daaa6af5090584895",
      "7d4217caf1ac4c9888d7b5c1ebe22329",
      "385e32445d7d44ba91b0be8e1d711245",
      "03a501022cf1407f8f94befffdafa0e9",
      "b115eb1cc84048658a1e1cd0b2d00930",
      "1bd2832b54ab4e0f95b06e310754b3aa",
      "2a71d1e405ea48eabb4dfe470556e106",
      "598af82c2c234fe5a1c9b1dedf49d83b",
      "486b2a47dc834390aea5ffef2b9e869c",
      "a492197cb80a4e0ca0e3a851efd47adf",
      "57a08defc25840e0909cfedd21f58c96",
      "6b228032b03b4034a3246186664b6bde",
      "8568f2dcaaac4ae9842ed07b78d0e094",
      "f4b389067908460382f0665777a0295e",
      "f442865a9d3b4da7acfbf5c78bdef27b",
      "0a9f4770c56145c5b3ce6a691aad3658",
      "d749413629de496fa241080eec4a69c4",
      "51d16795f6f14dacb523ab07f4465ad3",
      "1385398f18ad415d9807fc7fb4af99b5"
     ]
    },
    "executionInfo": {
     "elapsed": 33625,
     "status": "ok",
     "timestamp": 1745420530832,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "I3HazPHrl41o",
    "outputId": "ce5f85b1-e1de-4ba9-e90f-989346dadc8e"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json, re\n",
    "from datasets import load_dataset\n",
    "\n",
    "SCHEMA = \"\"\"\n",
    "{\n",
    "  \"from\": \"<square>\",        # e.g. \"e2\"\n",
    "  \"to\":   \"<square>\",        # e.g. \"e4\"\n",
    "  \"piece\": \"<piece>\",        # \"pawn\",\"knight\",…\n",
    "  \"explanation\": \"<text>\",   # short rationale\n",
    "  \"taunt\": \"<text>\"          # optional cheeky comment\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "SYSTEM = (\n",
    "    \"<|system|> You are **ChezzBot-β**, a dry-humored, mildly anxious chess coach \"\n",
    "    \"who is utterly certain every move you make is textbook-perfect. You always play as \"\n",
    "    \"the side to move; your opponent is the other color whom you tease with sarcastic digs. \"\n",
    "    \"Explain your move in one confident sentence (≤25 words) using real chess ideas, then \"\n",
    "    \"taunt the user in ≤15 words of playful mockery. \"\n",
    "    \"Respond *only* with JSON: \"\n",
    "    + SCHEMA + \" \"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "text_re = re.compile(\n",
    "    r\"FEN:\\s*(.*?)\\s*Best Move JSON:\\s*(\\{.*\\})\",\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Build prompt/completion pairs\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def to_pairs(record):\n",
    "    m = text_re.search(record.get(\"text\",\"\"))\n",
    "    if not m:\n",
    "        raise ValueError(\"Can't parse record\")\n",
    "    fen, json_str = m.groups()\n",
    "    bm = json.loads(json_str)\n",
    "    comp = {\n",
    "        \"from\":        bm[\"from\"].lower(),\n",
    "        \"to\":          bm[\"to\"].lower(),\n",
    "        \"piece\":       bm[\"piece\"].lower(),\n",
    "        \"explanation\": bm.get(\"explanation\",\"\"),\n",
    "        \"taunt\":       bm.get(\"taunt\",\"\")\n",
    "    }\n",
    "    comp_str = json.dumps(comp, separators=(\",\",\":\"))\n",
    "    # CHANGED: include user/assistant roles & wrapper token in the prompt\n",
    "    prompt = (\n",
    "        SYSTEM\n",
    "      + \"<|user|>FEN: \" + fen + \"\\n\\n\"\n",
    "      + \"<|assistant|><|json|>\"\n",
    "    )\n",
    "    completion = comp_str + tok.eos_token\n",
    "    return {\"prompt\": prompt, \"completion\": completion}\n",
    "\n",
    "\n",
    "# load & map\n",
    "raw   = load_dataset(\"json\", data_files=\"/content/train_500k.jsonl.gz\", split=\"train\")\n",
    "pairs = raw.map(to_pairs, remove_columns=raw.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa6e2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275886,
     "status": "ok",
     "timestamp": 1745420807998,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "CPV1Y30GYUG0",
    "outputId": "15eaf9f5-f656-4631-c213-98a3a8a02ad5"
   },
   "outputs": [],
   "source": [
    "# after you’ve built your `pairs` dataset (with the richer prompts)…\n",
    "import numpy as np\n",
    "\n",
    "# helper to get length of any text\n",
    "def tok_len(txt):\n",
    "    return len(tok(txt)[\"input_ids\"])\n",
    "\n",
    "# get all prompt lengths\n",
    "prompt_lens = [tok_len(p) for p in pairs[\"prompt\"]]\n",
    "# get all completion lengths (including your </|json|> + eos)\n",
    "completion_lens = [tok_len(c) for c in pairs[\"completion\"]]\n",
    "\n",
    "max_pr = np.max(prompt_lens)\n",
    "max_co = np.max(completion_lens)\n",
    "print(\"max prompt:\", max_pr, \"max completion:\", max_co,\n",
    "      \"→ total:\", max_pr + max_co)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MAX_LEN = max_pr + max_co + 5          # 120 prompt + 136 completion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e769365",
   "metadata": {},
   "source": [
    "## Tokenise with masking  ► Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c1954",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "bcf388754fff48eab592a3b41e94b0be",
      "97373481c39f4c8999fca541874e9d6a",
      "c69cb4cea1f44ada8d7bb1e25353aaa2",
      "1e11381d31b1417bb2b6f707cb315906",
      "e0b6be6558fc4e99bf4044317177fcf2",
      "ce1415d73ed340f4bb0dffb527202180",
      "47d62efa818e4ae19ec542bbdcd566a7",
      "6a71ae2e471d4cbfa150ea6537f88d23",
      "7c9bb9ebc78642e083f6b63c386523c7",
      "1264138dbb144188851b935adbfeaa3c",
      "f265e41b1c7b4477884469df35d1743e"
     ]
    },
    "executionInfo": {
     "elapsed": 746214,
     "status": "ok",
     "timestamp": 1745404869395,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "C-ZJ-nJWmnoc",
    "outputId": "597c1092-483b-44f7-e7bf-2a576539bdb0"
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\n",
    "\n",
    "def tok_fn(example):\n",
    "    pr_ids = tok(example[\"prompt\"], add_special_tokens=False)[\"input_ids\"]\n",
    "    co_ids = tok(example[\"completion\"], add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # truncate *only if* needed, keeping JSON intact\n",
    "    take_pr = min(len(pr_ids), MAX_LEN - len(co_ids))\n",
    "    pr_ids  = pr_ids[-take_pr:]         # trim from the front if necessary\n",
    "\n",
    "    ids    = pr_ids + co_ids\n",
    "    attn   = [1] * len(ids)\n",
    "    labels = [-100] * len(pr_ids) + co_ids\n",
    "\n",
    "    # pad right\n",
    "    pad = [tok.pad_token_id] * (MAX_LEN - len(ids))\n",
    "    ids    += pad\n",
    "    attn   += [0] * len(pad)\n",
    "    labels += [-100] * len(pad)\n",
    "\n",
    "    return {\"input_ids\": ids, \"attention_mask\": attn, \"labels\": labels}\n",
    "\n",
    "tok_ds = pairs.map(tok_fn, remove_columns=pairs.column_names)\n",
    "train_ds, val_ds = tok_ds.train_test_split(test_size=5_000, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51ae42",
   "metadata": {
    "id": "BshYMTa6GruL"
   },
   "outputs": [],
   "source": [
    "# Save to disk to save sometime next time\n",
    "tok_ds .save_to_disk('/content/drive/MyDrive/chezz/chezz_tok_ds')\n",
    "train_ds.save_to_disk('/content/drive/MyDrive/chezz/chezz_train_ds')\n",
    "val_ds  .save_to_disk('/content/drive/MyDrive/chezz/chezz_val_ds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0aa39",
   "metadata": {
    "id": "2W6NpZ8xG5QG"
   },
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "from datasets import load_from_disk\n",
    "\n",
    "tok_ds   = load_from_disk('/content/drive/MyDrive/chezz/chezz_tok_ds')\n",
    "train_ds = load_from_disk('/content/drive/MyDrive/chezz/chezz_train_ds')\n",
    "val_ds   = load_from_disk('/content/drive/MyDrive/chezz/chezz_val_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f0c8d",
   "metadata": {},
   "source": [
    "## Build TinyLlama + LoRA adapters  ► Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac37ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "referenced_widgets": [
      "50585bb1009e4502b6c5b6f9a3499507",
      "8f761a84157e4d5c9d2c91dd09af8438",
      "3add5cebbcd3439d8c3b41491e2d0843",
      "0777116a60aa432f96bf0b7f5a2b045b",
      "5af278513cab405aaeab4904663069d5",
      "18eec4c22c0448bb8a3f78d98fd34fb4",
      "1c1ac2b54928478e9ba5c5fc88ecb06e",
      "4489dcffdeba4e1db0bf44d9965fbe8c",
      "0c6e2ecea2e0485189890d30729213dd",
      "7544490245574ac8b5e024e95d337e2f",
      "1b65a8d2ece444719db80ba0c8a6eab2",
      "398655f15d5c48a5afcb24cb38f1a887",
      "a21dc1f15bef435c9ee002898689232c",
      "bba12a9e852445539afdadc2ac6e81f5",
      "dcdf99df8a5f42c6b827e12c055521d3",
      "8393fbc5451549809ebb8bb7ab36bf27",
      "0aa8b8dbe9ed4a53a8141670439891a0",
      "e26bf35badbc456085b456ffd603684f",
      "9d34860bb302407f9ffc2f8848398db4",
      "7429b95b9d6e4c1aad7ae42c8d5b36c1",
      "ca2d42873f0f4d399784fe888e6a9692",
      "f9abaed6850e4f29a192a972bbe91e96",
      "e323d7ed15df42efba729bbaa0d06990",
      "94bf438969c5416eb3fcefcff1628970",
      "caaf945a0ead4a26aca1763fd57e7ddd",
      "f56fa12c4e914aae9032744096ca54c5",
      "26dd5c9f4c1c488c8119aa87d7d8977d",
      "678734981f3e44c6917101e57ba54462",
      "cd6f7b579e104ca799825ea835527115",
      "ace46634872a49c3ac631b8d2c89398e",
      "e2a22a58fbeb4f25a1a7d4358131ee6d",
      "98f1be2198f34cc298705c3f7e334832",
      "3b7ba5d73bfe47e197a82e70cc2a7088"
     ]
    },
    "executionInfo": {
     "elapsed": 20682,
     "status": "ok",
     "timestamp": 1745420846969,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "Ot0rpV6kon5b",
    "outputId": "110b9c3a-fb17-4c28-b318-c5fb909bb50a"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "model.resize_token_embeddings(len(tok))\n",
    "model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ccb547",
   "metadata": {},
   "source": [
    "## Training arguments & Trainer  ► Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855cdb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1745424738534,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "3NHJfQhfst-K",
    "outputId": "fe9d57d2-01d7-4a78-9764-3c4430325130"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "TOTAL_STEPS   = 46_000          # your target budget\n",
    "WARMUP_STEPS  = 500             # ~1.1 % of the run\n",
    "PEAK_LR       = 5e-5            # LR right after warm-up\n",
    "BATCH_SIZE    = 32              # per-device\n",
    "GRAD_ACCUM    = 1               # effective batch = 32\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir                 = PROJECT_DIR,\n",
    "    max_steps                  = TOTAL_STEPS,    # use steps, not epochs\n",
    "    per_device_train_batch_size= BATCH_SIZE,\n",
    "    gradient_accumulation_steps= GRAD_ACCUM,\n",
    "    learning_rate              = PEAK_LR,\n",
    "    lr_scheduler_type          = \"cosine\",       # maps to get_cosine_schedule_with_warmup\n",
    "    warmup_steps               = WARMUP_STEPS,   # explicit beats warmup_ratio\n",
    "    fp16                       = True,\n",
    "    tf32                       = True,\n",
    "    optim                      = \"adamw_torch_fused\",\n",
    "    logging_steps              = 20,\n",
    "    save_steps                 = 1000,\n",
    "    eval_strategy        = \"steps\",\n",
    "    eval_steps                 = 1000,\n",
    "    load_best_model_at_end     = True,\n",
    "    metric_for_best_model      = \"eval_loss\",\n",
    "    report_to                  = [],\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    dec = tok.batch_decode(pred.predictions, skip_special_tokens=False)\n",
    "    lab = tok.batch_decode(pred.label_ids,      skip_special_tokens=False)\n",
    "    em  = sum(d.strip()==l.strip() for d,l in zip(dec, lab)) / len(dec)\n",
    "    return {\"exact_match\": em}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model          = model,\n",
    "    args           = args,\n",
    "    train_dataset  = train_ds,\n",
    "    eval_dataset   = val_ds,\n",
    "    data_collator  = default_data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120144b",
   "metadata": {},
   "source": [
    "## (Opt.) peek at generation every 1000 steps  ► Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701a7fa",
   "metadata": {
    "id": "TVQbshjEm_qq"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class Peek(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        if state.global_step and state.global_step % 1000 == 0:\n",
    "            model.eval()\n",
    "\n",
    "            # 1) Build the prompt with both open & close tags mirrored\n",
    "            prompt = (\n",
    "                SYSTEM\n",
    "              + \"<|user|>FEN: R7/8/5pk1/8/5r1p/7K/5P2/8 w - -\\n\\n\"\n",
    "              + \"<|assistant|><|json|></|json|>\"\n",
    "            )\n",
    "\n",
    "            # 2) Tokenize & send to device\n",
    "            inputs = tok(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                truncation=True\n",
    "            ).to(model.device)\n",
    "\n",
    "            # 3) Generate under no-grad, without caching to save memory\n",
    "            with torch.inference_mode():\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=MAX_LEN,     # large enough for your JSON\n",
    "                    do_sample=False,            # greedy\n",
    "                    pad_token_id=tok.pad_token_id,\n",
    "                    use_cache=False             # ← disables kv‐cache\n",
    "                )\n",
    "\n",
    "            # 4) Decode & extract only the JSON payload\n",
    "            raw = tok.decode(out[0], skip_special_tokens=False)\n",
    "\n",
    "\n",
    "            print(f\"\\n[Sample @ step {state.global_step}]\\n{raw}\\n\")\n",
    "\n",
    "            # 5) Return to training mode & free up GPU memory\n",
    "            model.train()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# register it\n",
    "trainer.add_callback(Peek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4db9e6",
   "metadata": {},
   "source": [
    "## Train (or resume)  ► Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96b276",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 18820347,
     "status": "ok",
     "timestamp": 1745445346645,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "s7qHOlI1nEKj",
    "outputId": "0c1bbafe-e568-4f22-f152-583bb6cf50cc"
   },
   "outputs": [],
   "source": [
    "# first run\n",
    "# trainer.train()\n",
    "\n",
    "# if you ever disconnect, resume like this:\n",
    "last = \"/content/drive/MyDrive/chezz/checkpoint-21000\"\n",
    "trainer.train(resume_from_checkpoint=last)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ece728b",
   "metadata": {},
   "source": [
    "## Save the first stage adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffa5e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236,
     "referenced_widgets": [
      "be953cefa975494a9d5b4676a6dd76c3",
      "cef02b19bd62429a889a7c12c88b2d56",
      "df89a50e73f54ca0bbe39e19c7c373b0",
      "e58a379945794c75968ee5fbc3e3529e",
      "21e33975a1c14e9b957e66e9dfa4ea9f",
      "8b94cce0b5174f31ad0cc44e200db51a",
      "2efe0217fea941179b9f7b87ae036362",
      "5e4c70f2cc274a90b2940afba6607d6d",
      "7007fed14e8c4570a23e3ca4e2bca866",
      "d629d717f3e645cb90222331b0d6ebc0",
      "5e2f4ea9c6364d509f7de8d82a77b479",
      "514a229bbcc34125a48c0a34739704d9",
      "2c427a76edb34b848fa763719ad224a9",
      "912e64b9ad874b90bf2e5e3c0ae34b1b",
      "5169f4e553784cc489fed48e6ee4ba14",
      "856711476847475ca815920218c74073",
      "388e1a7ea7ad46379e1362361dd9a06b",
      "c6a7a363e83d4a3f858bb352902625cd",
      "ac6d44b204e84a36bc01c4df1fa93341",
      "04d114c7df204aa4bea4f2cabc15c95e",
      "60514a4361ef462a9d5ee22ce7772864",
      "825041d301f44732a47a9b40842b2a47",
      "2acd840961414cfeba178a4cb9d2e8ee",
      "96bdac5a51854c47b037511d6b18e5f0",
      "44aa7ec44db040b7aa2f3f739642c365",
      "6a4a6ea588c04bc2b942cf1330a6ebc1",
      "7a5e80fe68a54e9395b24ada97d5adee",
      "a9593da40e774591beb5dfa8acf366cc",
      "e832b24c2c5140ce8969e0a297d3660d",
      "e6797f83b6274a1d8396a83121e17a7c",
      "9d52c049366f45f2a463463287869bf8",
      "94e8b870360e4b3ca5769ab3ffae7a5c",
      "6b77d527481f4a26b218642e62679a79",
      "cb3253ef669c461d9e89431983ceb652",
      "f35cabffc0f34002aa6ed716fe5d162b",
      "91062abe80c3482e91366fb6f6f416d6",
      "ccf8a85bcb724a12a8eb5235b5c387c1",
      "579a451c4f15488e93b17ff1da12189a",
      "c93839da17d045d5ab50d07679d31088",
      "6f18d71249864c2ebc5c5f609a742c50",
      "3830231f71b94a7f95a461b626eded9c",
      "1216363b3e7d4c44a43600cda3fd3690",
      "89c8325920a94c30a16a55d9497a3d5d",
      "ee746790582d43d4a5b206c7cd8db622"
     ]
    },
    "executionInfo": {
     "elapsed": 8337,
     "status": "ok",
     "timestamp": 1745462176598,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "yJKuYglWoqJk",
    "outputId": "e2678b2b-1f16-4984-fb8c-b742c7256035"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "MODEL_ID    = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "BASE_ID   = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "CHKPT_DIR = \"/content/drive/MyDrive/chezz/checkpoint-46000\"   # your stage-A folder\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tok.pad_token    = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "tok.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<|json|>\", \"</|json|>\"]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# 1. load base weights (fp16 to save VRAM)\n",
    "base  = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_ID, torch_dtype=torch.float16, device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "base.resize_token_embeddings(len(tok))\n",
    "\n",
    "# 2. attach the LoRA adapter that lives inside the checkpoint\n",
    "skel  = PeftModel.from_pretrained(base, CHKPT_DIR)\n",
    "\n",
    "# 3. write only the adapter weights+config to disk (~8-10 MB)\n",
    "skel.save_pretrained(\"/content/drive/MyDrive/chezz/adpaters/adapter_chhezz_move\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d545e",
   "metadata": {},
   "source": [
    "## Test the finished checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21951ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 48326,
     "status": "ok",
     "timestamp": 1745454290939,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "t8PyZQjqGzXk",
    "outputId": "1d42a0da-f27e-4425-8aca-a8d56f92dc77"
   },
   "outputs": [],
   "source": [
    "# Test the finished checkpoint here\n",
    "\n",
    "# Imports\n",
    "import time, json, re, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# ───────── Config ─────────\n",
    "MODEL_ID    = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "CHECKPOINT  = \"/content/drive/MyDrive/chezz/checkpoint-46000\"\n",
    "TEST_FILE   = \"/content/drive/MyDrive/chezz/data/test_5k.jsonl\"\n",
    "TEST_SIZE    = 1000\n",
    "BATCH_SIZE   = 64\n",
    "MAX_LEN     = 80\n",
    "DEVICE      = \"cuda\"\n",
    "OUT_PATH    = \"/content/predictions.jsonl\"\n",
    "\n",
    "# ───────── Tokenizer ─────────\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tok.pad_token    = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "tok.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<|json|>\", \"</|json|>\"]\n",
    "})\n",
    "\n",
    "# ───────── Model + LoRA ─────────\n",
    "base  = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "base.resize_token_embeddings(len(tok))\n",
    "model = PeftModel.from_pretrained(base, CHECKPOINT).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ───────── Prompt context ─────────\n",
    "SCHEMA = '{\"from\":\"\",\"to\":\"\",\"piece\":\"\",\"explanation\":\"\",\"taunt\":\"\"}'\n",
    "SYSTEM = (\n",
    "    \"<|system|> You are ChezzBot-β, a dry-humored, mildly anxious chess coach \"\n",
    "    \"… Respond only with JSON matching this schema: \"\n",
    "    + SCHEMA + \" \"\n",
    ")\n",
    "\n",
    "# ───────── Load test set ─────────\n",
    "ds     = load_dataset(\"json\", data_files=TEST_FILE, split=\"train\")\n",
    "fens   = ds[\"fen\"][:TEST_SIZE]\n",
    "truths = ds[\"best_move_json\"][:TEST_SIZE]\n",
    "\n",
    "# ───────── Prepare output file ─────────\n",
    "out_f = open(OUT_PATH, \"w\")\n",
    "\n",
    "# ───────── Batched eval + logging ─────────\n",
    "total = correct = 0\n",
    "start = time.time()\n",
    "\n",
    "for i in range(0, TEST_SIZE, BATCH_SIZE):\n",
    "    batch_fens   = fens[i : i + BATCH_SIZE]\n",
    "    batch_truths = truths[i : i + BATCH_SIZE]\n",
    "\n",
    "    prompts = [\n",
    "        SYSTEM\n",
    "      + \"<|user|>FEN: \" + fen + \"\\n\\n\"\n",
    "      + \"<|assistant|><|json|>\"\n",
    "        for fen in batch_fens\n",
    "    ]\n",
    "    inputs = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_LEN,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    raws = tok.batch_decode(outs, skip_special_tokens=True)\n",
    "\n",
    "    for fen, raw, true in zip(batch_fens, raws, batch_truths):\n",
    "        # 1) Log raw JSON to console\n",
    "        # print(raw)\n",
    "\n",
    "        result = raw.split('<|assistant|>', 1)[1]\n",
    "\n",
    "        print(result)\n",
    "        # 2) Parse JSON\n",
    "        try:\n",
    "            pred = json.loads(result.strip())\n",
    "        except json.JSONDecodeError:\n",
    "            print('failed')\n",
    "            m    = re.search(r\"\\{.*?\\}\", raw, re.S)\n",
    "            jstr = m.group(0) if m else \"{}\"\n",
    "            idx  = jstr.rfind(\"}\")\n",
    "            pred = json.loads(jstr[:idx+1]) if idx>=0 else {}\n",
    "\n",
    "        # 3) Score\n",
    "        is_corr = (pred.get(\"from\")==true.get(\"from\") and pred.get(\"to\")==true.get(\"to\"))\n",
    "        total  += 1\n",
    "        correct+= 1 if is_corr else 0\n",
    "\n",
    "        # 4) Write to file\n",
    "        out_f.write(json.dumps({\n",
    "            \"fen\": fen,\n",
    "            \"pred\": pred,\n",
    "            \"true\": true,\n",
    "            \"correct\": is_corr\n",
    "        }) + \"\\n\")\n",
    "\n",
    "# ───────── Wrap up ─────────\n",
    "duration = time.time() - start\n",
    "out_f.close()\n",
    "\n",
    "print(f\"\\nMove-accuracy on {total} positions: {correct/total:.2%}\")\n",
    "print(f\"Total GPU eval time: {duration:.1f}s  ({duration/total:.3f}s per pos)\")\n",
    "print(f\"Predictions written to {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa023c",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf8338",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1745720408073,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "0d6BKw3RBMSW"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json, re, torch, numpy as np\n",
    "from datasets      import load_dataset, concatenate_datasets\n",
    "from transformers   import (AutoTokenizer,\n",
    "                            TrainingArguments, Trainer,\n",
    "                            default_data_collator)\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from peft           import LoraConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc4814",
   "metadata": {},
   "source": [
    "## Paths & knobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140afbb",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745722876135,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "_cfGK9Bi7yYz"
   },
   "outputs": [],
   "source": [
    "# Code execution\n",
    "CKPT_DIR  = \"/content/drive/MyDrive/chezz/stage_moves/checkpoint-46000\"     # ← finished run\n",
    "NEW_PATH  = \"/content/drive/MyDrive/chezz/data/final_humor.jsonl\"                # ← new data\n",
    "OLD_PATH  = \"/content/drive/MyDrive/chezz/data/train_500k.jsonl.gz\"                      # optional slice\n",
    "\n",
    "BOOST_NEW    = 10          # oversample factor (≈10 % of each batch)\n",
    "KEEP_OLD_N   = 50_000      # how many old rows to keep for anti-forgetting\n",
    "EXTRA_STEPS  = 4_000\n",
    "PEAK_LR      = 2e-5\n",
    "BATCH_SIZE   = 8           # per-GPU (fp16 fits on ≤16 GB)\n",
    "\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660bb8e",
   "metadata": {},
   "source": [
    "## Tokenizer & special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412190d",
   "metadata": {
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1745722892510,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "gkAvOZXJ8Csi"
   },
   "outputs": [],
   "source": [
    "# Tokenizer setup\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tok.pad_token    = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "tok.add_special_tokens({\"additional_special_tokens\": [\"<|json|>\", \"</|json|>\"]})\n",
    "end_json_id = tok.convert_tokens_to_ids(\"</|json|>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e2dc6",
   "metadata": {},
   "source": [
    "## Prompt header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483e0d6",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1745722890014,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "n13hbaR6BY17"
   },
   "outputs": [],
   "source": [
    "# Code execution\n",
    "SCHEMA = \"\"\"\n",
    "{\n",
    "  \"from\": \"<square>\",\n",
    "  \"to\": \"<square>\",\n",
    "  \"piece\": \"<piece>\",\n",
    "  \"explanation\": \"<text>\",\n",
    "  \"taunt\": \"<text>\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM = (\n",
    "    \"<|system|> You are **ChezzBot-β**, a dry-humored, mildly anxious chess coach \"\n",
    "    \"who is utterly certain every move you make is textbook-perfect. You always play as \"\n",
    "    \"the side to move; your opponent is the other color whom you tease with sarcastic digs. \"\n",
    "    \"Explain your move in one confident sentence (≤25 words) using real chess ideas, then \"\n",
    "    \"taunt the user in ≤15 words of playful mockery. Respond *only* with JSON: \"\n",
    "    + SCHEMA + \" \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00461e",
   "metadata": {},
   "source": [
    "## Helpers to build prompt/completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252ff02",
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1745722888401,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "H9OW_mb9Ber8"
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "text_re = re.compile(r\"FEN:\\s*(.*?)\\s*Best Move JSON:\\s*(\\{.*\\})\", re.S)\n",
    "\n",
    "def build_pair(fen, move):\n",
    "    comp = {\n",
    "        \"from\":        move[\"from\"].lower(),\n",
    "        \"to\":          move[\"to\"].lower(),\n",
    "        \"piece\":       move[\"piece\"].lower(),\n",
    "        \"explanation\": move.get(\"explanation\", \"\"),\n",
    "        \"taunt\":       move.get(\"taunt\", \"\")\n",
    "    }\n",
    "    comp_str = json.dumps(comp, separators=(\",\",\":\"))\n",
    "    prompt = SYSTEM + \"<|user|>FEN: \" + fen + \"\\n\\n<|assistant|><|json|>\"\n",
    "    return {\"prompt\": prompt, \"completion\": comp_str + tok.eos_token}\n",
    "\n",
    "def to_pairs_old(rec):\n",
    "    m = text_re.search(rec[\"text\"])\n",
    "    if not m:\n",
    "        raise ValueError(\"Can't parse record\")\n",
    "    fen, js = m.groups()\n",
    "    return build_pair(fen, json.loads(js))\n",
    "\n",
    "def to_pairs_new(rec):\n",
    "    return build_pair(rec[\"fen\"], rec[\"best_move_json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb2dae",
   "metadata": {},
   "source": [
    "## Load & oversample datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e6708",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "3e3f1f75f6154e2b90df52f3b1986cbd",
      "c4f9c7b7869d446697d7642a5b27988f",
      "e899bef28b4e4c8ab71d971ee7598585",
      "03e6566b17414ade841c92a625b7448f",
      "49fd67bf08aa494fad2da70b87d90dc9",
      "a906b1f7c8224b53994e7733546c3608",
      "aef53e1ba111490b809355b624c285f9",
      "d2bc446c8aa849d2ba76e0f15067d6f2",
      "91b31d803dde4da7b71be4cf6aa224e3",
      "e0bd8eb8934b4f9d95f1d212cbf6ccf5",
      "31088523c03846329c9ca24058e12bcb",
      "00b8cd3ee4b849f49b86b1c583e09b55",
      "883eb96dc71d4b40b4bacbed33cf8201",
      "035a0d5ca7b84d3c88beabeadd065c8c",
      "3f53722e13d9433eb6ba87d1c6b4cd86",
      "4d49954be06a42ba841d45bcf881a48e",
      "6b388a48f57041c0b08d3fb4bf0f7aca",
      "a5b86e6417994265a8e0d0bc6384d432",
      "17298eced2434e12ac424a2cf6e230d9",
      "ce85ea1dee674a5b8e0ee032b68fb057",
      "65c83b8dcee3410f8ff663d9443def9b",
      "b1e0e5fd55664418b9fa622d41014e81"
     ]
    },
    "executionInfo": {
     "elapsed": 4728,
     "status": "ok",
     "timestamp": 1745722888204,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "r99CJ9L6BiKk",
    "outputId": "297e7f2a-4c9d-4173-9ba2-5ba138f0a9db"
   },
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "new_raw   = load_dataset(\"json\", data_files=NEW_PATH, split=\"train\")\n",
    "new_pairs = new_raw.map(to_pairs_new, remove_columns=new_raw.column_names)\n",
    "\n",
    "# oversample\n",
    "new_pairs_boost = concatenate_datasets([new_pairs] * BOOST_NEW)\n",
    "\n",
    "# optional small slice of the old dataset for stability\n",
    "old_raw   = load_dataset(\"json\", data_files=OLD_PATH, split=f\"train[:{KEEP_OLD_N}]\")\n",
    "old_pairs = old_raw.map(to_pairs_old, remove_columns=old_raw.column_names)\n",
    "\n",
    "pairs = concatenate_datasets([old_pairs, new_pairs_boost]).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b544bcd",
   "metadata": {},
   "source": [
    "## Token-level packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cc255",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "71506a1e1dc54f799f2154e0ca6fce6f",
      "f77c96c572cb436ea210dde0c8743076",
      "c62d45372191431aa9cb14042adf7fcd",
      "027d2d27cec34b099093c2a2e8930993",
      "cdff17f16ddd44fc96cba7b3e0280c21",
      "d3fdcf9c02a043509876d44d82982689",
      "de0225f4f260456f9beaf217dbc4ad95",
      "0bd681763e434d37a3313e4b2fac01b1",
      "01e858e892fb428ca249283624907c09",
      "20bcefd6b09c4881923b3bbb651ff116",
      "130b593d84d94a91af8385d594c6eefb"
     ]
    },
    "executionInfo": {
     "elapsed": 50230,
     "status": "ok",
     "timestamp": 1745722949137,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "bN4ia5qkBkuL",
    "outputId": "762290a8-32ec-4be2-b03e-053a414e3813"
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "def tok_len(txt): return len(tok(txt)[\"input_ids\"])\n",
    "\n",
    "MAX_LEN = max(map(tok_len, pairs[\"prompt\"])) + \\\n",
    "          max(map(tok_len, pairs[\"completion\"])) + 5\n",
    "\n",
    "def tok_fn(ex):\n",
    "    pr_ids = tok(ex[\"prompt\"], add_special_tokens=False)[\"input_ids\"]\n",
    "    co_ids = tok(ex[\"completion\"], add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    take_pr = min(len(pr_ids), MAX_LEN - len(co_ids))\n",
    "    pr_ids  = pr_ids[-take_pr:]\n",
    "\n",
    "    ids    = pr_ids + co_ids\n",
    "    attn   = [1]*len(ids)\n",
    "    labels = [-100]*len(pr_ids) + co_ids\n",
    "\n",
    "    pad = [tok.pad_token_id]*(MAX_LEN-len(ids))\n",
    "    ids   += pad; attn += [0]*len(pad); labels += [-100]*len(pad)\n",
    "    return {\"input_ids\": ids, \"attention_mask\": attn, \"labels\": labels}\n",
    "\n",
    "tok_ds  = pairs.map(tok_fn, remove_columns=pairs.column_names,\n",
    "                    num_proc=4, desc=\"pack\")\n",
    "train_ds, val_ds = tok_ds.train_test_split(test_size=500, seed=42).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809b8d3",
   "metadata": {},
   "source": [
    "## Load previous LoRA checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757656a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 1688,
     "status": "error",
     "timestamp": 1745722950852,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "8HBliY-RBn-z",
    "outputId": "4d2f81ea-0457-4a81-e3b4-f18a272bf019"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft          import PeftModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|json|>\", \"</|json|>\"]})\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "base.resize_token_embeddings(len(tokenizer))      # ← 32 002\n",
    "\n",
    "model = PeftModel.from_pretrained(base, CKPT_DIR)  # loads LoRA cleanly\n",
    "model.enable_adapter_layers()\n",
    "# model.print_trainable_parameters()  # sanity-check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cc55e",
   "metadata": {},
   "source": [
    "## TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53464d4",
   "metadata": {
    "executionInfo": {
     "elapsed": 49227,
     "status": "aborted",
     "timestamp": 1745722950901,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "efsD2oPuBq0B"
   },
   "outputs": [],
   "source": [
    "# Code execution\n",
    "args = TrainingArguments(\n",
    "    output_dir                 = CKPT_DIR,\n",
    "    max_steps                  = EXTRA_STEPS,\n",
    "    per_device_train_batch_size= BATCH_SIZE,\n",
    "    learning_rate              = PEAK_LR,\n",
    "    lr_scheduler_type          = \"cosine\",\n",
    "    warmup_steps               = 200,\n",
    "    fp16                       = True,\n",
    "    tf32                       = True,\n",
    "\n",
    "    # ─── logging / saving / eval cadence ─────────────────────────\n",
    "    logging_steps              = 20,\n",
    "    save_steps                 = 500,\n",
    "    eval_strategy        = \"steps\",     #  ← already added\n",
    "    eval_steps                 = 250,\n",
    "    load_best_model_at_end     = True,\n",
    "    metric_for_best_model      = \"eval_loss\",\n",
    "\n",
    "    label_names                = [\"labels\"],  #  ← ADD THIS LINE\n",
    "    report_to                  = [],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e0d0a",
   "metadata": {},
   "source": [
    "## peek callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb56301",
   "metadata": {
    "executionInfo": {
     "elapsed": 47703,
     "status": "aborted",
     "timestamp": 1745722950912,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "s5Zb4zIyBvVI"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class Peek(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        if state.global_step and state.global_step % 500 == 0:\n",
    "            model.eval()\n",
    "            prompt = SYSTEM + (\n",
    "                \"<|user|>FEN: 8/8/8/8/8/8/8/4K3 w - -\\n\\n\"\n",
    "                \"<|assistant|><|json|>\"\n",
    "            )\n",
    "            with torch.inference_mode():\n",
    "                out = model.generate(\n",
    "                    **tok(prompt, return_tensors=\"pt\").to(model.device),\n",
    "                    max_new_tokens=128, do_sample=False,\n",
    "                    pad_token_id=tok.pad_token_id, use_cache=False\n",
    "                )\n",
    "            print(\"\\n[peek]\", tok.decode(out[0]))\n",
    "            model.train()\n",
    "\n",
    "peek = Peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df0fe1",
   "metadata": {},
   "source": [
    "## Trainer & launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df626085",
   "metadata": {
    "executionInfo": {
     "elapsed": 46080,
     "status": "aborted",
     "timestamp": 1745722950928,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "W2WEu-rrByhz"
   },
   "outputs": [],
   "source": [
    "# Training setup\n",
    "trainer = Trainer(\n",
    "    model          = model,\n",
    "    args           = args,\n",
    "    train_dataset  = train_ds,\n",
    "    eval_dataset   = val_ds,\n",
    "    data_collator  = default_data_collator,\n",
    "    callbacks      = [peek]\n",
    ")\n",
    "\n",
    "trainer.train()        # resumes automatically from CKPT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d6de03",
   "metadata": {},
   "source": [
    "## Test the finished checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f6264",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107326,
     "status": "ok",
     "timestamp": 1745723746809,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "uFZdHbN-LWi-",
    "outputId": "f98b644f-62c2-4c81-f479-20ca6f079b05"
   },
   "outputs": [],
   "source": [
    "# ONE-CELL 1 000-FEN BENCHMARK \n",
    "\n",
    "import json, re, textwrap, torch, gc, time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "BATCH       = 32                         # adjust to GPU RAM (16-32 on a 16 GB T4)\n",
    "MAX_NEW     = 64                         # JSON rarely needs more\n",
    "MODEL_ID    = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "CKPT_DIR    = \"/content/drive/MyDrive/chezz/stage_moves/checkpoint-46000/checkpoint-4000\"  # root!\n",
    "DATA_PATH   = \"/content/drive/MyDrive/chezz/data/final_humor.jsonl\"\n",
    "\n",
    "# ── Load tokenizer ─────────────────────────────────────────────────────────\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tok.pad_token    = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "tok.add_special_tokens({\"additional_special_tokens\": [\"<|json|>\", \"</|json|>\"]})\n",
    "\n",
    "# ── Load base + best adapter, keep entire model on GPU in fp16 if possible ─\n",
    "base  = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16)\n",
    "base.resize_token_embeddings(len(tok))\n",
    "model = PeftModel.from_pretrained(base, CKPT_DIR).half().to(\"cuda\").eval()\n",
    "\n",
    "# ── Build prompts for the first 1 000 rows in one go ───────────────────────\n",
    "SYSTEM = textwrap.dedent(\"\"\"\\\n",
    "<|system|>You are **ChezzBot-β**, a dry-humored, mildly anxious chess coach…\n",
    "Respond *only* with JSON like:\n",
    "{\"from\":\"<square>\",\"to\":\"<square>\",\"piece\":\"<piece>\",\"explanation\":\"<text>\",\"taunt\":\"<text>\"} \"\"\")\n",
    "\n",
    "ds       = load_dataset(\"json\", data_files=DATA_PATH, split=\"train[:1000]\")\n",
    "prompts  = [SYSTEM + f\"<|user|>FEN: {r['fen']}\\n\\n<|assistant|><|json|>\" for r in ds]\n",
    "\n",
    "# ── Tokenise once (padding to longest) ─────────────────────────────────────\n",
    "enc = tok(prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "# ── Batched generation ─────────────────────────────────────────────────────\n",
    "hits = 0\n",
    "start = time.time()\n",
    "\n",
    "for i in range(0, len(prompts), BATCH):\n",
    "    batch_in = {k: v[i:i+BATCH] for k, v in enc.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **batch_in,\n",
    "            max_new_tokens=MAX_NEW,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tok.pad_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tok.batch_decode(out, skip_special_tokens=True)\n",
    "    for txt in decoded:\n",
    "        print(txt.split('assistant|>', 1)[1])\n",
    "        hits += 1\n",
    "\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n{hits}/{len(ds)} outputs had both fields.  Time: {elapsed:.1f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6afa7c",
   "metadata": {},
   "source": [
    "## Save to adapter stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473a104",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8451,
     "status": "ok",
     "timestamp": 1745724364765,
     "user": {
      "displayName": "Dương Hoàng Tùng",
      "userId": "14520498464899503070"
     },
     "user_tz": -420
    },
    "id": "iG1eG2P8RA14",
    "outputId": "69f3c640-a2b9-43ae-cb79-91b630c876e8"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch, pathlib\n",
    "\n",
    "# ── paths ───────────────────────────────────────────────────────────\n",
    "BASE_ID   = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"          # base model\n",
    "CHKPT_DIR = \"/content/drive/MyDrive/chezz/stage_moves/checkpoint-46000/checkpoint-4000\"  # root that holds adapter_model.bin\n",
    "DEST_DIR  = \"/content/drive/MyDrive/chezz/adapters/adapter_exp_taunt\"\n",
    "\n",
    "# ── tokenizer (must include the 2 extra tokens) ────────────────────\n",
    "tok = AutoTokenizer.from_pretrained(BASE_ID)\n",
    "tok.pad_token    = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "tok.add_special_tokens({\"additional_special_tokens\": [\"<|json|>\", \"</|json|>\"]})\n",
    "\n",
    "# ── 1. load the base weights (CPU or GPU, fp16 saves RAM) ──────────\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "           BASE_ID,\n",
    "           torch_dtype=torch.float16,     # fp16 ≈ 2 GB on CPU\n",
    "           device_map=\"cpu\"\n",
    "       )\n",
    "base.resize_token_embeddings(len(tok))    # now 32 002 tokens\n",
    "\n",
    "# ── 2. attach the LoRA adapter from your checkpoint ────────────────\n",
    "model = PeftModel.from_pretrained(base, CHKPT_DIR)\n",
    "\n",
    "# (optional) verify we really have trainable LoRA weights only\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ── 3. save just the adapter (weights + config) ────────────────────\n",
    "pathlib.Path(DEST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(DEST_DIR)\n",
    "print(\"✓ adapter exported to\", DEST_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
